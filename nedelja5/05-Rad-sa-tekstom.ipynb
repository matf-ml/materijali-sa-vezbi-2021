{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rad sa tekstom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U zajednici koja se bavi obradom prirodnih jezika (engl. Natural Language Processing) postoji nekoliko popularnih paketa među kojima svakako prednjače [NLTK](https://www.nltk.org/), [SpaCy](https://spacy.io/) i [FastText](https://fasttext.cc/). U primerima koji sledi biće opisana NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK paket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK je paket koji ima najdužu istoriju i koji nudi najviše mogućnosti u radu sa tekstom. Koriste ga lingvisti, istraživači koji se bave digitalnom humanistikom, kao i istraživači u domenu obrade prirodnih jezika. Više o samom paketu i njegovim funkcionalnostima se može naći na [zvaničnom sajtu](https://www.nltk.org/), a nadalje slede primeri nekoliko najvažnijih i najčešće korišćenih metoda. Alat podržava rad sa velikim brojem jezika, a primeri koji slede će se odnositi na engleski jezik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK paket se može instalirati pomoću conda `alata` komandom `conda install -c anaconda nltk` u skladu sa [zvaničnim smernicama](https://anaconda.org/anaconda/nltk). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/andjelka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/andjelka/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekst koji ćemo koristiti za vežbu preuzet je sa Vikipedijinog članka o Nikoli Tesli na engleskom jeziku.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"Born and raised in the Austrian Empire, Mr. Tesla studied engineering and physics in the 1870s without receiving a degree, and gained practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry. He emigrated in 1884 to the U.S.A., where he became a naturalized citizen. He worked for a short time at the Edison Machine Works in New York City before he struck out on his own.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Born and raised in the Austrian Empire, Mr. Tesla studied engineering and physics in the 1870s without receiving a degree, and gained practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry. He emigrated in 1884 to the U.S.A., where he became a naturalized citizen. He worked for a short time at the Edison Machine Works in New York City before he struck out on his own.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inteligentna podela teksta na rečenice se postiže pozivom metode `sent_tokenizer`. Kao separatori se koriste znaci interpunkcije poput tačke, upitnika ili uzvičnika. Metoda je sposobna da razna pojave ovih karatkera i u drugim kontekstima i pridruži im ispravnu funkciju. Na primer, tačka koja je sastavni deo skraćenica U.S.A. ili datuma 20.02.2020. neće uticati na pogrešno razbijanje rečenice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Born and raised in the Austrian Empire, Mr. Tesla studied engineering and physics in the 1870s without receiving a degree, and gained practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry.',\n",
       " 'He emigrated in 1884 to the U.S.A., where he became a naturalized citizen.',\n",
       " 'He worked for a short time at the Edison Machine Works in New York City before he struck out on his own.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rečenice se dalje mogu deliti na tokene. To se postiže pozivom metode `word_tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'He worked,  for a short time, at the Edison Machine Works in New York City before he struck out on his own.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'worked',\n",
       " ',',\n",
       " 'for',\n",
       " 'a',\n",
       " 'short',\n",
       " 'time',\n",
       " ',',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Edison',\n",
       " 'Machine',\n",
       " 'Works',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'City',\n",
       " 'before',\n",
       " 'he',\n",
       " 'struck',\n",
       " 'out',\n",
       " 'on',\n",
       " 'his',\n",
       " 'own',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovako izdvojeni tokeni se dalje mogu filtrirati. Na primer, mogu se eliminisati znaci interpunkcije ili reči koje se jako često pojavljuju i koje svojim značenjem ne doprinose zadatku. Takve reči se zovu funkcijske ili stop reči i za svaki jezik postoji lista reči koja se ubraja u ovu kategoriju. Na primer, za engleski jezik to su članovi a i the, predlozi poput in, at, on i slično. Sam izbor kriterijuma filtriranja pre svega zavisi od zadatka i ne može se grubo generalizovati. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svi interpunkcijski karakteri \n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punctuation = [token for token in tokens if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'worked',\n",
       " 'for',\n",
       " 'a',\n",
       " 'short',\n",
       " 'time',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Edison',\n",
       " 'Machine',\n",
       " 'Works',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'City',\n",
       " 'before',\n",
       " 'he',\n",
       " 'struck',\n",
       " 'out',\n",
       " 'on',\n",
       " 'his',\n",
       " 'own']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_stopwords = [token for token in tokens if token not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'worked',\n",
       " ',',\n",
       " 'short',\n",
       " 'time',\n",
       " ',',\n",
       " 'Edison',\n",
       " 'Machine',\n",
       " 'Works',\n",
       " 'New',\n",
       " 'York',\n",
       " 'City',\n",
       " 'struck',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizacija i stemovanje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nakon filtriranja, tokeni se dalje mogu dalje obrađivati. Na primer, tokeni poput plays, played, playing i playing bi mogli da se dovedu u vezu sa infinitivom glagola play, a tokeni poput apples ili oranges sa imenicama jednine apple i orange. U zavisnosti da li se ovo uvezivanje vrši po lingvističkim pravilima ili primenom heuristika, možemo govoriti o `lematizaciji` ili `stemovanju`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porterov stemer je jedan od najpoznatijih stemera za engleski jezik. Njime se dosledno primenjuje niz pravila kako bi se dobio `stem` - veštački koren reči. Na primer, neka pravila su odsecanje ed sufiksa (played->play), zamena sufiksa ational sufiksom ate (relational->relate) ili ization sufiksom ize (organization->organize). Osim ovakvih pravila postoje i pravila koja se zasnivaju na analizi vokala i konsonanata. O njima i samom algorimu se može pročitati više u [zvaničnoj dokumentaciji](https://www.nltk.org/_modules/nltk/stem/porter.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('played')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['played', 'playing', 'plays', 'interesting', 'apples', 'books', 'are', 'is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['play', 'play', 'play', 'interest', 'appl', 'book', 'are', 'is']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neke popularne alternative Porterovom stemeru su SnowballStemmer, LancasterStemmer, RegexpStemmer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za razliku od stemovanja, lematizacijom se tokenu pridružuje gramatički koren (takozvana lema). Na primer, glagolu are se pridružuje lema be, a imenici boxes lema box. NLTK paket nudi mogućnost rada sa [WordNet](https://wordnet.princeton.edu/) lematizatorom, manualno kreiranom leksičkom bazom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('are', pos = 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'box'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('boxes', pos = 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uz token, lematizer očekuje i informaciju o vrsti reči (eng. part-of-speach) kao aproksimaciju konteksta u kojem se token nalazi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za automatsko dobijanje vrste reči, mogu se koristiti takozvani POS tager. POS tageri pridružuju tokenima odgovarajuće POS obeležje. To su obično klasifikatori za labeliranje sekvenci trenirani na velikoj kolekciji obeleženog teksta tako da maksimizuju verovatnoće pojavljivanja određenih sekvenci. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('worked', 'VBD'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('short', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " (',', ','),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Edison', 'NNP'),\n",
       " ('Machine', 'NNP'),\n",
       " ('Works', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP'),\n",
       " ('City', 'NNP'),\n",
       " ('before', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('struck', 'VBD'),\n",
       " ('out', 'RP'),\n",
       " ('on', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('own', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tager podrazumevano koristi takozvani PennTreebank skup tagova. U njemu, na primer, tag NNP označava vlastitu imenicu, NN imenicu, DT član, IN predlog... Detaljan opis ove sheme tagova se može pronaći [ovde](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da bi mogli da spojimo POS tager sa lematizacijom, moramo uskladiti tagove koji se pridružuju tokenima. WordNet lematizator koristi svoj skromniji skup tagova  (\"a\", \"s\", \"r\", \"n\", \"v\" redom sa znacenjima ADJ, ADJ_SAT, ADV, NOUN, VERB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    # uparujemo tagove\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"S\": wordnet.ADJ_SAT,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    # izdvajamo prvi karakter taga koji bi pridruzio POS tager\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "    # mapiramo ga u odgovarajuci WordNet tag ili postavljamo podrazumevano obelezje imenice\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'work',\n",
       " ',',\n",
       " 'for',\n",
       " 'a',\n",
       " 'short',\n",
       " 'time',\n",
       " ',',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Edison',\n",
       " 'Machine',\n",
       " 'Works',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " 'City',\n",
       " 'before',\n",
       " 'he',\n",
       " 'struck',\n",
       " 'out',\n",
       " 'on',\n",
       " 'his',\n",
       " 'own',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_tokens = []\n",
    "for token in tokens: \n",
    "    postag = get_wordnet_pos(token)\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token, postag))\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da li ćemo iskoristiti lematizaciju ili stemovanje zavisi od prirode zadatka koji rešavamo, kao i resura kojima raspolažemo. Lematizacija zahteva postojanje leksičke baze nalik WordNetu i njeno korišćenje može da uspori program, dok steming daje nešto nepreciznije rezultate ali brže. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematizacija i stemovanje se ubrajaju u zadatke `normalizacije` teksta tj. reči. U zavisnosti od tipa tekstualnog sadržaja mogu postojati potrebe i za drugim vrstama normalizacije. Na primer, jezik na društvenim mrežama je često vrlo specifičan, obiluje mnoštvom skraćenica i reči koje odstupaju od standardnog pravopisa. Tako se recimo skraćenica u2 se svodi na 'you too', skraćenica tmrw na 'tomorrow', a cooool na 'cool'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prethodno opisani postupci nam omogućavaju da kreiramo vokabular tj. skup svih reči koji se nalazi u nekom tekstu ili kolekciji tekstova (korpusu). Nad vokabularom dalje kreiramo atribute potrebne za rad primenu algoritama mašinskog učenja. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
