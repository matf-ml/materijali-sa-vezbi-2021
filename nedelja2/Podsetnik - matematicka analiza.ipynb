{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matematicka analiza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Izvod funkcije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za funkciju $f: R \\rightarrow R$ izvod funkcije $f^\\prime$ u tacki $x$ definisemo sa \n",
    "$$f^\\prime (x) =  lim_{\\Delta x \\rightarrow 0} \\frac{f(x+\\Delta x) - f(x)}{\\Delta x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficki, izvod funkcije u $f$ u tacki $x$ predstavlja koeficijent pravca tangente u tacki $x$. <img src='assets/derivate.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer.\n",
    "\n",
    "Izvod funkcije $f(x) = 2\\cdot x^2 - x$ po definiciji mozemo izracunati na sledeci nacin: \n",
    "$$f^\\prime (x) =  lim_{\\Delta x \\rightarrow 0} \\frac{f(x+\\Delta x) - f(x)}{\\Delta x} = $$\n",
    "$$lim_{\\Delta x \\rightarrow 0} \\frac{ (2\\cdot(x+\\Delta x)^2 - (x + \\Delta x)) - (2\\cdot x^2 - x)}{\\Delta x} = $$\n",
    "$$lim_{\\Delta x \\rightarrow 0} \\frac{ 4x\\Delta x + 2\\Delta^2 x  - \\Delta x }{\\Delta x} = $$\n",
    "$$lim_{\\Delta x \\rightarrow 0} {( 4x + 2\\Delta x  - 1)} = $$\n",
    "$$ 4x - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izvodi nekih funkcija: \n",
    "* $(x^n)^\\prime = n x^{n-1}$\n",
    "* $sin(x)^\\prime = cos(x)$\n",
    "* $cos(x)^\\prime = -sin(x)$\n",
    "* $(e^x)^\\prime = e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pravila koja vaze prilikom izracunavanja izvoda: \n",
    "* aditivnost: $(f+g)^\\prime = f^\\prime + g^\\prime$\n",
    "* proizvod: $(f \\cdot g)^\\prime = f^\\prime \\cdot g + f \\cdot g^\\prime$\n",
    "* kompozicija: $ (g \\circ f)^ \\prime (x) = (g(f(x))^\\prime = g^\\prime(f(x))\\cdot f^\\prime(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U kontekstu masinskog ucenja, izvodi su od sustinskog znacaja kod gradijentnih metoda optimizacije. Njih koristimo za obucavanje modela. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parcijalni izvod funkcije"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za funkciju $f: R^n \\rightarrow R$ vise promenljivih, uvodimo pojam parcijalnog izvoda $$\\frac{\\partial f(x_1, x_2, \\ldots x_n)}{\\partial {x_i}} = lim_{h \\rightarrow 0} \\frac{f(x_1, x_2, \\ldots, x_{i-1}, x_i + h, x_{i+1},\\ldots, x_n) - f(x_1, \\ldots, x_{i-1}, x_i, x_{i+1}, \\ldots, x_n)}{h}\\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer.\n",
    "\n",
    "Parcijalni izvodi funkcije $f(x_1, x_2) = e^{x_1} + x_2$ su \n",
    "$f^\\prime_{x_1} = \\frac{\\partial f(x_1, x_2)}{\\partial x_1} = e^{x_1}$\n",
    "$f^\\prime_{x_2} = \\frac{\\partial f(x_1, x_2)}{\\partial x_2} = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parcijalni izvodi funkcije $f(x_1, x_2) = e^{x_1^2 + sin(x_2)}$ su \n",
    "$\\frac{\\partial f(x_1, x_2)}{\\partial x_1} = e^{x_1^2 + sin(x_2)} \\cdot 2\\cdot x_1$ i \n",
    "$\\frac{\\partial f(x_1, x_2)}{\\partial x_2} = e^{x_1^2 + sin(x_2)} \\cdot cos(x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradijent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradijent funkcije $f: R^n \\rightarrow R$ je vektor svih parcijalnih izvoda $\\nabla f(x_1, \\ldots, x_n) = [\\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n}] $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer:\n",
    "Gradijent funkcije $f(x_1, x_2) = e^{x_1^2 + sin(x_2)}$ je $\\nabla f = [2\\cdot x_1 \\cdot e^{x_1^2 + sin(x_2)},  cos(x_2) \\cdot e^{x_1^2 + sin(x_2)} ] $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/gradient.png' style='width: 500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hesijan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hesijan funkcije $f: R^n \\rightarrow R$ je matrica svih drugih parcijalnih izvoda funkcije $\\nabla^2 f(x_1, \\ldots, x_n) = [\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer. \n",
    "\n",
    "Za funkciju $f(x_1, x_2) = e^{x_1} + x_2$ izracunali smo parcijalne izvode prvog reda: $\\frac{\\partial f(x_1, x_2)}{\\partial x_1} = e^{x_1}$ i $\\frac{\\partial f(x_1, x_2)}{\\partial x_2} = 1$.\n",
    "\n",
    "Parcijalni izvodi drugog reda su: \n",
    "$\\frac{\\partial^2 f(x_1, x_2)}{\\partial x_1\\partial x_1} = e^{x_1}$\n",
    "$\\frac{\\partial^2 f(x_1, x_2)}{\\partial x_1\\partial x_2} = 0$\n",
    "$\\frac{\\partial^ f(x_1, x_2)}{\\partial x_2 \\partial x_1} = 0$\n",
    "$\\frac{\\partial^ f(x_1, x_2)}{\\partial x_2 \\partial x_2} = 0$\n",
    "\n",
    "Stoga je Hesijan $\\nabla^2 f = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial^2 f(x_1, x_2)}{\\partial x_1\\partial x_1} & \\frac{\\partial^2 f(x_1, x_2)}{\\partial x_1\\partial x_2} \\\\\n",
    "\\frac{\\partial^ f(x_1, x_2)}{\\partial x_2 \\partial x_1} & \\frac{\\partial^ f(x_1, x_2)}{\\partial x_2 \\partial x_2} \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "e^{x_1} & 0 \\\\\n",
    "0 & 0 \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konveksnost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neka je X konveksan skup i neka je $f$ realna funkcija definisana na njemu. Za funkciju $f$ kazemo da je **konveksna** ukoliko za svako $\\alpha \\in [0, 1]$ i svako $x_1, x_2 \\in X$ vazi $ f(\\alpha x_1+ (1-\\alpha) x_2) \\le \\alpha f(x_1) + (1-\\alpha) f(x_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realna funkcija $f$ je **konkavna** ukoliko je funkcija $-f$ konveksna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukoliko u nejednakostima koje definisu konveksnosti i konkavnosti vazi stroga nejednakost, govorimo o **strogo konveksnim** i **strogo konkavnim** funkcijama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teorema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za funkciju $f: R^n \\rightarrow R$ koja je diferencijabilna u tacki $x$ vazi f je konveksna akko postoji okolina tacke $x$ takva da za svako $y$ iz te okoline vazi $f(y) \\ge f(x) + \\nabla f(x)^T \\cdot (y-x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrijska interpretacija ove teoreme je:  grafik funkcije je iznad tangentne ravni.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dva puta diferencijabilna funkcija $f$ je strogo konveksna u nekoj tacki $x$ ukoliko je hesijan $\\nabla^2f(x)$ pozitivno definitan u nekoj okoline te tacke. Takodje, hesijan funkcije koja je konveksna u nekoj tacki ima pozitivnu vrednost u nekoj okolini te tacke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konveksne funkcije karakterisu i sledeca svojstva:\n",
    "- linearna kombinacija konveksnih funkcija je konveksna funkcija\n",
    "- ako je $f$ konveksna funkcija onda je i $f(Ax+b)$ za proizvoljnu matricu $A$ i vektor $b$ odgovarajucih dimenzija konveksna funkcija\n",
    "- $max \\{f_1, \\ldots, f_n\\}$ je konveksna funkcija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaka konveksnost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcija $f$ koja je diferencijabilna u tacki $x$ je jako konveksna ukoliko za svako $y$ iz okoline tacke $x$ vazi: $f(y) \\ge f(x) + \\nabla f(x)^T \\cdot (y-x) + \\frac{m}{2}||x-y||$ za neko $m \\gt 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcije koje imaju svojstvo jake konveksnosti se bolje ponasaju prilikom primene algoritama optimizacije baziranim na gradijentoj metodi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teorema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dva puta diferencijabilna funkcija $f$ je jako konveksna u tacki $x$ ako je matrica $\\nabla^2f(x) - mI$ pozitivno semidefinitna za neko $m \\gt 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukoliko u praksi treba proveriti da li je funkcija konveksna, obicno: \n",
    "\n",
    "1) proveravamo uslov definicije konveksnosti\n",
    "\n",
    "2) proveravamo uslove koji vaze sa hesijan\n",
    "\n",
    "3) utvrdjujemo da je funkcija dobijena operacijama koje cuvaju konveksnost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lokalni i globalni optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za funkciju $f$ kazemo da ima **lokalni minimum** u tacki $x$ ukoliko u nekoj okolini te tacke uzima vrednosti vece od $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za funkciju $f$ kazemo da ima **strogi lokalni minimum** u tacki $x$ ukoliko u nekoj okolini te tacke uzima vrednosti strogo vece od $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogno uvodimo pojmove **lokalni maksimum** i **strogi lokalni maksimum**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Globalni minimum** funkcije je minimum svih lokalnih minimuma, a **globalni maksimum** maksimum svih lokalnih maksimuma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/global_and_local_optima.png' style='width: 500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum i maksimum funkcije jednim imenom zovemo **optimumima** funkcije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teorema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako je $x_0$ optimum funkcije $f$ i funkcije $f$ je diferencijabilna u tacki $x_0$, tada vazi $\\nabla f(x_0)=0$. \n",
    "\n",
    "Pritom vazi i: \n",
    "- ako je $f(x_0)$ minimum, onda je funkcija konveksna u tacki $x_0$\n",
    "- ako je $f(x_0)$ maksimum, onda je funckija konkavna u tacki $x_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
